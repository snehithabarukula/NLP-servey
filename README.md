# Literature Survey on N-gram Models in NLP

## Overview
This literature survey explores the application and development trends of N-gram models in Natural Language Processing (NLP). Conducted as part of the CS6120 course at Northeastern University during Summer 2024, this project reviews the pivotal role of N-gram models in enhancing various NLP tasks such as sentiment analysis, text classification, and predictive text systems.

## Objectives
- **To understand the current state of N-gram models in NLP**: Analyzing how these models have been adapted and integrated with other machine learning techniques to tackle modern challenges in NLP.
- **To highlight significant advancements**: Focusing on innovations that have increased the accuracy and efficiency of these models in real-world applications.
- **To identify future research directions**: Suggesting areas where N-gram models can be further optimized or innovatively applied.

## Methodology
- **Extensive Literature Review**: Over 20 recent scholarly articles and conference papers were reviewed, focusing on the use of N-gram models in various NLP applications. 
- **Data Analysis Techniques**: Utilized statistical and machine learning algorithms to analyze the effectiveness of N-gram models in the surveyed literature.
- **Collaborative Effort**: This survey was a collaborative effort involving multiple contributors, which ensured a comprehensive analysis and review.

## Key Findings
- **Hybrid Models**: N-gram models combined with techniques like Na√Øve Bayes and deep learning have shown improved performance in sentiment analysis and text classification.
- **Innovative Applications**: The survey covers novel uses of N-gram models in areas such as malware detection, language translation, and even quantum computing.
- **Challenges and Gaps**: Identifies the limitations of current N-gram models and suggests areas for future research, particularly in enhancing model scalability and handling large datasets.

## Future Research Directions
The survey suggests further exploration into:
- **Minority Language Modeling**: Developing robust N-gram models for languages with limited digital resources.
- **Quantum Computing Integration**: Harnessing quantum computing to enhance the computational efficiency of N-gram models.
- **Interdisciplinary Applications**: Applying N-gram models across different fields such as cybersecurity and healthcare.

## Conclusion
This literature survey provides valuable insights into the effectiveness and versatility of N-gram models in NLP, underlining their importance in the ongoing evolution of language processing technologies.
